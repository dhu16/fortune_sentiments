{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjNjk2kmcX1nyFgsaRbmqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhu16/fortune_sentiments/blob/main/Week8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR4K38GZ6THG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jepjwybX6llQ"
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y95B3_bm-aGy"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0T_B85h-cZD"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh3FLQHe6sSV"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.rc('xtick', labelsize=14) \n",
        "matplotlib.rc('ytick', labelsize=14)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnJPf83f-EcI"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1O6RaEYB3Pu8y1IyLbJPZTSRTN9ekgN7-\"})\n",
        "downloaded.GetContentFile('fortune_phrases.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERz2mtzI70jf"
      },
      "source": [
        "#Create dataframe with only 1 column containing the sentences\n",
        "df = pd.read_excel('fortune_phrases.xlsx', usecols=['phrase'])\n",
        "\n",
        "#Convert dataframe to list\n",
        "content=df.stack().tolist()\n",
        "content[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvMnUpjyGehG"
      },
      "source": [
        "#Using nltk for stop words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd9vqSHGHsnY"
      },
      "source": [
        "def full_remove(x, removal_list):\n",
        "    for w in removal_list:\n",
        "        x = x.replace(w, ' ')\n",
        "    return x\n",
        "\n",
        "#Remove punctuation\n",
        "removePunc = [full_remove(x, list(string.punctuation)) for x in content]\n",
        "\n",
        "#Make everything lower-case and remove any white space\n",
        "sents_lower = [x.lower() for x in removePunc]\n",
        "sents_lower = [x.strip() for x in sents_lower]\n",
        "\n",
        "#Remove stop words\n",
        "stops = stopwords.words(\"english\")\n",
        "\n",
        "def removeStopWords(stopWords, txt):\n",
        "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
        "    return newtxt\n",
        "\n",
        "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
        "    \n",
        "processedSentences = [removeStopWords(stop_set,x) for x in sents_lower]\n",
        "processedSentences[0:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkzp2AkiJxAq"
      },
      "source": [
        "#Using TextBlob to determine polarity and subjectivity of sentences\n",
        "from textblob import TextBlob\n",
        "\n",
        "#Create polarity function and subjectivity function\n",
        "pol = lambda x: TextBlob(x).sentiment.polarity\n",
        "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "#Finds all words that can be assinged polarity and subjectivity and averages them together for the whole sentence's respective polarity/subjectivity\n",
        "pol_list = [pol(x) for x in processedSentences]\n",
        "sub_list = [sub(x) for x in processedSentences]"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVj-NAVWMxjH"
      },
      "source": [
        "#Append polarity and subjectivity sentiments to dataframe\n",
        "df['Polarity']=pol_list\n",
        "df['Subjectivity']=sub_list\n",
        "\n",
        "#Print dataframe\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}